---
title: Multivariate covariance generalized linear models for analysis of experimental
  data
author: "Phd. Wagner Hugo Bonat &  Dr. Walmes Marques Zeviani"
date: "July 24-28 2017"
output:
  slidy_presentation:
    css: config/style.css
    footer: McGLMs for experimental data , Bonat & Zeviani (2017)
    highlight: haddock
    includes:
      in_header: config/MathJax.html
  ioslides_presentation:
    highlight: haddock
  beamer_presentation:
    highlight: haddock
---

<!-- Não mexer aqui. -->
<style type="text/css">
body {
    font-family: "Palatino Linotype", "Book Antiqua", Palatino, serif;
}
img {
    box-shadow: 0 0 20px rgba(0, 0, 0, 0.75);
}
tt, code, pre {
    font-family: "Ubuntu Mono", "Inconsolata", "Andale Mono", monospace;
}
</style>
<!-- Não mexer aqui. -->


# Multivariate covariance generalized linear models for analysis of experimental data

## **Part II: Multivariate analysis of non-Gaussian data **

  - General setup
```{r}
data(iris)
head(iris)
```
  
  - MANOVA can deal only with Gaussian data.
  - Homogeneity of variances and covariances.
  - Independent observations within response variables.

---

## **Part II: Multivariate analysis of non-Gaussian data **

  - Goal 1: Extend the MANOVA to the class of multivariate covariance linear models (McLM) to deal with:
    - Repeated measures.
    - Times series.
    - Spatial and space-time data.
    - Genetic data.
  - Goal 2: Extend the McLM to the class of multivariate covariance generalized linear models (McGLM) to deal with:
    - Multivariate non-Gaussian data.
    - Response variables of mixed types.

---

### General linear models

  - Let $\mathrm{Y} = \{ \mathbf{Y}_1, \ldots, \mathbf{Y}_R \}$ denote
  the $N \times R$ matrix of response variables.
  - Let $\mathcal{Y} = (\mathbf{Y}_1^T, \ldots, \mathbf{Y}_R^T)^T$ denote 
  the stacked vector $NR \times 1$ of the response variable matrix 
  $\mathrm{Y}$ by columns.
  - Let $\mathbf{X} = \mathrm{Bdiag}(\mathrm{X}_1, \ldots, \mathrm{X}_R)$ 
  be an $(NR \times k)$ design matrix.
  - Let $\mathrm{M} = \{ \boldsymbol{\mu}_1, \ldots, \boldsymbol{\mu}_R \}$ be
  the $N \times R$ matrix of expected values.
  - Let $\mathcal{M} = (\boldsymbol{\mu}_1^T, \ldots, \boldsymbol{\mu}_R^T)^T$ be 
  the stacked vector $NR \times 1$ of the expected values matrix 
  $\mathcal{M}$ by columns.
  - General linear model
  
  $$ \mathcal{Y} \sim \mathrm{N}_{NR}(\mathcal{M}, \Sigma \otimes \mathrm{I})$$
  where $\mathrm{I}$ denotes an $(N \times N)$ identity matrix and $\otimes$ denotes
  the Kronecker product.

---

### Second-moments assumptions

  - Equivalently, we can specify the model using second-moments assumptions
  
  \begin{eqnarray}
  \mathrm{E}(\mathcal{Y}) &=& \mathcal{M} \\
  \mathrm{var}(\mathcal{Y}) &=& \Sigma \otimes \mathrm{I}.
  \end{eqnarray}
  
  - Expectation for each response variable (general linear models)
  
  $$ \boldsymbol{\mu}_r = \mathrm{X}_r \boldsymbol{\beta}_r. $$
  
  - We can easily extend to the class of general non-linear models
  
  $$ \boldsymbol{\mu}_r = f_{r}(\mathrm{X}_r; \boldsymbol{\beta}_r) $$
  where $f$ is a known non-linear function.
  
  - How to extend the general (non-)linear model to deal with 
  dependence within response variables?
  
---

### Multivariate linear covariance models (McLM)

  - It is clear that the assumptions of independence within observations
  appear in the covariance matrix through the identity matrix $\mathrm{I}$.
  - In order to deal with dependent observations, we have to allow non-diagonal matrices.
  - Decompose the $\Sigma$ matrix into marginal variances and correlations, i.e.
  
$$ 
  \Sigma = \begin{pmatrix}
\sqrt{\sigma_{11}} & 0 & \ldots & 0 \\ 
0 & \sqrt{\sigma_{22}} & \ldots & 0 \\    
\vdots &                & \ddots& \vdots \\
0      &  0 &  \ldots   & \sqrt{\sigma_{RR}}  
\end{pmatrix}
\begin{pmatrix}
1 & \rho_{12} & \ldots & \rho_{1R} \\ 
\rho_{21} & 1 & \ddots & \rho_{2R}\\ 
\vdots & \ddots & 1 &    \vdots \\ 
\rho_{R1} & \ldots & \ldots & 1
\end{pmatrix}
\begin{pmatrix}
\sqrt{\sigma_{11}} & 0 & \ldots & 0 \\ 
0 & \sqrt{\sigma_{22}} & \ldots & 0 \\    
\vdots &                & \ddots& \vdots \\
0      &  0 &  \ldots   & \sqrt{\sigma_{RR}}  
\end{pmatrix}
$$
where $\rho_{rr^{\prime}}$'s denote the correlations between the responses $r$ and $r^{\prime}$, for $r,r^{\prime} = 1, \ldots, R$.

  - Denote $\boldsymbol{\Sigma_b}$ the correlation matrix between response variables.
  - Denote $\boldsymbol{\Sigma_r} = \sigma_{rr} \mathrm{I}$ the covariance matrix
  within the response variable $r$.
  - Rewrite the covariance matrix using the generalized Kronecker product as
  
  $$ \Omega = \Sigma \otimes \mathrm{I} = \boldsymbol{\Sigma_R} \overset{G} \otimes \boldsymbol{\Sigma_b}, $$
  where
  $$ \boldsymbol{\Sigma}_R \overset{G} \otimes \boldsymbol{\Sigma}_b = \mathrm{Bdiag}(\tilde{\boldsymbol{\Sigma}}_1, \ldots, \tilde{\boldsymbol{\Sigma}}_R)(\boldsymbol{\Sigma}_b \otimes \boldsymbol{I})\mathrm{Bdiag}(\tilde{\boldsymbol{\Sigma}}_1^T, \ldots, \tilde{\boldsymbol{\Sigma}}_R^T).$$
  
  - $\tilde{\boldsymbol{\Sigma}}_r$ is the lower triangular matrix of the Cholesky decomposition of $\boldsymbol{\Sigma}_r$.
  - Generalized Kronecker product proposed by Martinez-Beneito (2013).
  - Note that the Cholesky decomposition corresponds to the square-root, since $\boldsymbol{\Sigma_r}$ is a diagonal matrix.
  - We model the within covariance matrix $\boldsymbol{\Sigma_r}$
  as a linear combination of known symmetric matrices, i.e.
  $$ \boldsymbol{\Sigma_r} = \tau_0 Z_0 + \ldots + \tau_{D} Z_D.$$
  

---

### Maximum likelihood estimation for the general linear model

  - Likelihood function
  
  $$ 
  L(\boldsymbol{\beta}, \Omega) = (2\pi)^{ -\frac{NR}{2} } |\Omega|^{-\frac{1}{2}} \exp \left \{ -\frac{1}{2}(\mathcal{Y} - \mathbf{X}\boldsymbol{\beta})^{\top}\Omega^{-1} (\mathcal{Y} - \mathbf{X}\boldsymbol{\beta}) \right \}.
  $$
  - Log-likelihood function
  
  $$ 
  l(\boldsymbol{\beta}, \Omega) = -\frac{NR}{2}\log(2\pi) -\frac{1}{2}\log |\Omega|  -\frac{1}{2}(\mathcal{Y} - \mathbf{X}\boldsymbol{\beta})^{\top}\Omega^{-1} (\mathcal{Y} - \mathbf{X}\boldsymbol{\beta}).
  $$
  
  - Score function for $\boldsymbol{\beta}$ is given by
  
  $$ 
  U_{\boldsymbol{\beta}}(\boldsymbol{\beta}, \Omega) = \frac{\partial l(\boldsymbol{\beta}, \Omega)}{\partial \boldsymbol{\beta}} = \mathbf{X}^{\top} \Omega^{-1}(\mathcal{Y} - \mathbf{X}\boldsymbol{\beta}).
  $$
  
  - Maximum likelihood estimator (MLE) for $\boldsymbol{\beta}$
  
  $$ 
  \hat{\boldsymbol{\beta}} = (\mathbf{X}^{\top}\mathbf{X})^{-1}\mathbf{X}^{\top}\mathcal{Y}. 
  $$
  
  - Fisher Information Matrix for $\boldsymbol{\beta}$
  
  $$ 
  \mathcal{F}_{\boldsymbol{\beta}} = \mathbf{X}^{\top}\Omega^{-1}\mathbf{X}.
  $$
  
  - Let $\boldsymbol{\sigma}$ be the stacked vector $(Q \times 1)$ 
  of the covariance parameters matrix $\Omega$ by columns.
  - We introduce the notation $\Omega(\boldsymbol{\sigma})$ to emphasize that
  $\Omega$ is composed by the components of $\boldsymbol{\sigma}$.
  - Score function for the component $\sigma_{i}$ is given by
  
  $$ U_{\sigma_{i}}(\boldsymbol{\beta}, \Omega(\boldsymbol{\sigma})) = 
  \frac{\partial l(\boldsymbol{\beta}, \Omega(\boldsymbol{\sigma}))}{\partial \sigma_{i}} = 
  -\frac{1}{2}\left \{ \Omega(\boldsymbol{\sigma})^{-1} - \Omega(\boldsymbol{\sigma})^{-1}(\mathcal{Y} - \mathbf{X}\boldsymbol{\beta})(\mathcal{Y} - \mathbf{X}\boldsymbol{\beta})^{\top}\Omega(\boldsymbol{\sigma})^{-1} \right \}
  \frac{\partial \Omega(\boldsymbol{\sigma})}{\partial \sigma_{i}}.
  $$
  
  - After some calculation, we can show that the MLE for $\Omega$ is given by
  
  $$ 
  \hat{\Omega} = \hat{\Sigma} \otimes \mathrm{I},
  $$
  where $\hat{\Sigma}$ is the empirical covariance matrix.
  - The $(i,j)$ entry of the Fisher Information matrix for the components $\sigma_i$ and $\sigma_j$
  is given by
  
  $$
  \mathcal{F}_{\sigma_i \sigma_j} = \frac{1}{2}\mathrm{tr}(\mathrm{W}_{\sigma_i}\Omega(\boldsymbol{\sigma}) \mathrm{W}_{\sigma_j}\Omega(\boldsymbol{\sigma})),
  $$
  where $\mathrm{W}_{\sigma_i} = -\frac{\partial \Omega(\boldsymbol{\sigma})^{-1}}{\partial \sigma_i}$.
  - Let $\mathcal{F}_{\boldsymbol{\sigma}}$ be the $Q \times Q$ Fisher information matrix
  for the vector $\boldsymbol{\sigma}$.
  - It is easy to show that $\boldsymbol{\beta}$ and $\boldsymbol{\sigma}$ are
  orthogonal. 
  - Fisher information matrix for $\boldsymbol{\theta} = (\boldsymbol{\beta}^{\top}, \boldsymbol{\sigma}^{\top})^{\top}$ is given by
  
  $$ 
  \mathcal{F}_{\boldsymbol{\theta}} = \begin{pmatrix} 
  \mathcal{F}_{\boldsymbol{\beta}} & \boldsymbol{0} \\  
  \boldsymbol{0} & \mathcal{F}_{\boldsymbol{\sigma}} 
  \end{pmatrix}.
  $$
  
  - Asymptotic distribution of the MLE
  
  $$ 
  \hat{\boldsymbol{\theta}} \sim N_{P+Q}(\boldsymbol{\theta},\mathcal{F}_{\boldsymbol{\theta}}^{-1}).
  $$

---

##  R code
```{r, include = TRUE, message = FALSE}
# Loading data set
data(iris)

# Fitting the model
fit1 <- lm(cbind(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) ~ Species, data = iris)

# Regression coefficients
coef(fit1)

# Covariance matrix
cov(residuals(fit1))

```
  - It provides exactly the same estimates that separate calls to the $\texttt{lm()}$ 
  function.
  - Standard errors for regression coefficients are available through
  the function $\texttt{vcov()}$.
  - Standard errors for covariance coefficients are not available.

---

### Multivariate hypotheses tests
  - Standard approaches are based on the decomposition of the total
  sum of squares and cross products (SSP) into regression and residuals, i.e.
  
  \begin{align}
  \mathrm{SSP}_{T} &=& \mathrm{Y}^{\top} \mathrm{Y} - N \bar{\mathrm{Y}}\bar{\mathrm{Y}}^{\top} \\
          &=& \hat{\mathrm{E}}^{\top} \hat{\mathrm{E}} + (\hat{\mathrm{Y}}^{\top} \hat{\mathrm{Y}} - N  \bar{\mathrm{Y}}\bar{\mathrm{Y}}^{\top}) \\
          &=& \mathrm{SSP}_{R} + \mathrm{SSP}_{Reg},
  \end{align}
  where $\bar{\mathrm{Y}}$ is the $(R \times 1)$ vector of means for
  the response variables.
  
  - $\hat{\mathrm{Y}} = \mathrm{X} \hat{\mathrm{B}}$ is the matrix of fitted values.
  - $\hat{\mathrm{E}} = \mathrm{Y} - \hat{\mathrm{Y}}$ is the matrix of residuals.
  - Let $\mathrm{SSP}_H$ denotes the incremental $\mathrm{SSP}$ matrix
  for a hypothesis.
  - Multivariate tests are based on the $R$ eigenvalues of $\mathrm{SSP}_H \mathrm{SSP}_R^{-1}$.
  - Let $\boldsymbol{\lambda}$ denote the eigenvalues of $\mathrm{SSP}_H \mathrm{SSP}_R^{-1}$, 
  that is, the values of $\boldsymbol{\lambda}$ for which
  
  $$ 
  | \mathrm{SSP}_H \mathrm{SSP}_R^{-1} - \boldsymbol{\lambda} \mathrm{I} | = 0.
  $$
  
  - The four most employed multivariate test statistics are functions
  of $\boldsymbol{\lambda}$:
    * Pillai-Bartlett Trace, $T_{PB} = \sum_{i = 1}^{R} \frac{\lambda_i}{(1- \lambda_i)}.$
    * Hotelling-Lawley Trace, $T_{HL} = \sum_{i = 1}^{R} \lambda_i.$
    * Wilk's Lambda, $\Lambda = \prod_{i=1}^{R} \frac{1}{1+\lambda_i}.$
    * Roy's maximum root, $T_{R} = \lambda_1.$
  - 
  
  
  
  
  
  





## Future research

  * Implement multivariate hypothesis tests for McGLM.
  * Explore the multiple design multivariate linear models.
  * A set of functions to communicate with `multcomp::glht()`.

## Next topics

  * Reapeted measures design.
  * Profile analysis.
  * Analysis for the canonical variates .
  * Multiple comparisons procedures.
  * Checking model assumptions.
  * Multiple design linear regression models.

---

<center>
<p style='font-size: 80px;'>Thank you!</p>
</center>
