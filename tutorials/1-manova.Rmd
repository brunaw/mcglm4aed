---
title: "Multivariate analysis of Gaussian data (MANOVA)"
subtitle: Part I
author: |
  | Wagner H. Bonat   | Walmes M. Zeviani |
  |:-----------------:|:-----------------:|
  | `wbonat@ufpr.br`  | `walmes@ufpr.br`  |
date: >
  62^a^ RBras & 17^o^ SEAGRO</br>
  July 24--28, 2017</br>
  UFLA, Lavras/MG
---

# Motivational data sets

## `iris` data set

  * `iris` data set was collected by Anderson (1935) on three
    species of irises in the Gasp√© Peninsula of Quebec, Canada.
  * This data set was employed by R. A. Fisher (1936)
    to introduce the method of discriminant analysis.
  * Easily available in $\textsf{R}$ through `data(iris)`.
  * Four response variables representing measurements (in cm) of parts
    of the flowers (length and width of sepal and petal).
  * One experimental factor with three levels `setosa`,
    `versicolor` and `virginica` representing the
    species of iris.
  * How the species are related with flowers size?
  * Size is not a really precise response variable and can be understood
    as a function of the four measured response variables.

```{r, include = FALSE, results = "hide", message = FALSE}
source("../slides/config/_setup.R")
library(lattice)
library(latticeExtra)
library(gridExtra)
library(ellipse)
library(corrplot)
library(car)
# library(mcglm)
```

```{r, include = TRUE, results = "hide", eval = FALSE}
data(iris)

scatterplotMatrix(~Sepal.Length + Sepal.Width +
                      Petal.Length + Petal.Width | Species,
                  data = iris,
                  smooth = FALSE,
                  reg.line = FALSE,
                  ellipse = TRUE,
                  by.groups = TRUE,
                  diagonal = "none",
                  legend.pos = "bottomleft")
```

```{r, include = TRUE, results = "hide"}
panel.ell <- function(x, y, ...) {
    panel.xyplot(x, y, ...)
    panel.ellipse(x, y, level = 0.5,
                  center.pch = 19, center.cex = 1, ...)
    panel.ellipse(x, y, level = 0.9,
                  center.pch = 19, center.cex = 1, ...)
}

splom(~iris[1:4],
      data = iris,
      as.matrix = TRUE,
      auto.key = list(columns = 3,
                      title = "Species",
                      cex.title = 1.1),
      groups = Species,
      lower.panel = panel.ell,
      upper.panel = panel.ell)
```

## `soya` data set

  * Experiment carried out in a vegetation house with soybeans.
  * Two plants by plot with three levels of the factor amount of
    water in the soil (`water`) and five levels of potassium
    fertilization (`pot`).
  * The plots were arranged in five blocks (`block`).
  * Three response variables were measured, namely, grain yield,
    number of seeds and number of viable peas per plant.
  * The main goal is to assess the effect of the experimental factors
    on the soya $\textbf{yield}$.
  * Soya $\textbf{yield}$ is only indirectly measured through the three
    measured response variables.
  * Response variables are of mixed types, i.e. grain yield is a
    continuous outcome while number of seeds and number of viable peas
    per plant are examples of count and binomial response variables.
  * Data set is available in $\textsf{R}$ through the `mcglm`
    package.

```{r, include = TRUE, results = "hide", message = FALSE}
data(soybeanwp, package = "wzRfun")
str(soybeanwp)

soybeanwp$viab <- with(soybeanwp, nvp/(nvp + nip))

splom(~soybeanwp[, c(1, 4, 7, 10)],
      #~soybeanwp[, c(1, 4:7, 10)],
      groups = soybeanwp$water,
      as.matrix = TRUE,
      lower.panel = function(x, y, groups, ...) {
          panel.xyplot(x, y, groups = groups, ...)
          i <- 0
          by(cbind(x, y),
             INDICES = groups,
             FUN = function(m) {
                 i <<- i + 1
                 panel.smoother(x = m[, 1],
                                y = m[, 2],
                                span = 0.9,
                                alpha.se = 0.15,
                                col = trellis.par.get()$superpose.line$col[i])
             })
      },
      auto.key = list(columns = 3,
                      title = "Water content (%)",
                      cex.title = 1.1))
```

```{r}
# str(soybeanwp)

# Fitting the same model to all response variables.
m0 <- lm(cbind(yield, w100, Kconc, tg, viab) ~
             block + water * factor(potassium),
         data = soybeanwp)

r <- residuals(m0)
```

```{r, include = FALSE, eval = FALSE}
# TODO fazer essa coisa acontecer!
# panel.qqmath.splom <- function(x, ...) {
#     yr <- current.panel.limits()$ylim
#     qq <- qqnorm(x, plot.it = FALSE)
#     u <- ((qq$y - min(qq$y))/diff(range(qq$y)))
#     y <- diff(yr) * u + min(yr)
#     # print(current.panel.limits()$xlim)
#     # print(range(x))
#     panel.xyplot(qq$x, y = y, ...)
# }

# splom(~r,
#       lower.panel = panel.ell,
#       upper.panel = panel.ell,
#       diag.panel = panel.qqmath.splom)
```
```{r}
# Change the 3rd color of the palette used for the ellipses.
oldpal <- palette()
palette(c("black", "blue", "red"))
scatterplotMatrix(r,
                  gap = 0,
                  smooth = FALSE,
                  reg.line = FALSE,
                  ellipse = TRUE,
                  diagonal = "qqplot")
palette(oldpal)
```

## Motivation and research questions

  * Models of nature and human behavior must often account for multiple,
    inter-related variables that are conceptualized simultaneously or
    over time.
  * Often in the same experiment more than one response variables are
    measured simultaneously.
  * Multivariate experiments are common when we have a set of variables
    to describe an attribute or latent variable.
    - Size variables (weight, height, width, diameter, etc).
    - Soil contents (P, K, pH, Ca, Mg, CTC, etc).
    - Personality traits (example TODO).
  * In general, the research questions are related to all or a
    combination of the response variables.
  * Examples of research questions.
    - Which is the best (set of) variable to measure?
    - What are the main effects of the covariates or experimental
      factors?
    - What are the interaction among experimental factors?
    - What is the strenght of association/correlation between response
      variables?
    - Which response variables are more affected by the experimental
      factors?

# Part I: Multivariate analysis of Gaussian data

## General multivariate linear model

### Matrix notation

$$
\mathrm{Y} = \mathrm{X}\mathbf{B} + \mathbf{E}
$$

$$
\begin{pmatrix}
    Y_{11} & \ldots  & Y_{1R}\\
    \vdots & \ddots  & \vdots \\
    Y_{N1} & \ldots  & Y_{NR}
  \end{pmatrix} =
  \begin{pmatrix}
    x_{11} & \ldots & x_{1k}\\
    \vdots & \ddots & \vdots \\
    x_{N1} & \ldots & x_{Nk}
  \end{pmatrix}
  \begin{pmatrix}
    \beta_{01} & \ldots & \beta_{0R}\\
    \vdots     & \ddots & \vdots \\
    \beta_{k1} & \ldots & \beta_{kR}
  \end{pmatrix} +
  \begin{pmatrix}
    \epsilon_{11} & \ldots & \epsilon_{1R}\\
    \vdots        & \ddots & \vdots \\
    \epsilon_{N1} & \ldots & \epsilon_{NR}
  \end{pmatrix}
$$


  * $\mathrm{Y}$ is a matrix of $N$ observations on $R$ response
    variables.
  * $\mathrm{X}$ is an $(N \times k)$ model matrix with collumns for $k$
    regressors (experimental factors).
  * $\mathrm{B}$ is a $(k \times R)$ matrix of regression coefficients.
  * $\mathrm{E}$ is an $(N \times R)$ matrix of errors,
    such that $\epsilon^{\top}_i \sim N_{R}(\mathbf{0}, \Sigma)$.
  * $\Sigma$ is an ($R \times R$) covariance matrix, constant across
    observations, i.e.

$$
\Sigma =
  \begin{pmatrix}
    \sigma_{11} & \ldots & \sigma_{1R} \\
    \vdots      & \ddots & \vdots \\
    \sigma_{R1} & \ldots & \sigma_{RR}
  \end{pmatrix}.
$$

### Least squares estimators for regression coefficients

$$
\hat{\mathrm{B}} =
  (\mathrm{X}^{\top}\mathrm{X})^{-1}
  \mathrm{X}^{\top}\mathrm{Y}.
$$

$\Sigma$ can be estimated by the empirical covariance matrix

$$
\hat{\Sigma} =
  \frac{(\mathrm{Y} - \mathrm{X} \hat{\mathrm{B}})^{\top}
  (\mathrm{Y} - \mathrm{X} \hat{\mathrm{B}})}{n}.
$$

## R code

```{r, include = TRUE, message = FALSE}
data(iris)
head(iris)

# Response variable matrix.
# Y <- as.matrix(iris[, 1:4], ncol = 4, nrow = 150)
Y <- as.matrix(iris[, 1:4])
dim(Y)

# Design matrix.
X <- model.matrix(~Species, data = iris)

# Regression coefficients (don't try this at home, children).
B <- solve(tcrossprod(t(X))) %*% t(X) %*% Y
B

# Covariance matrix.
# Sigma <- t(Y - X %*% B) %*% (Y - X %*% B)/nrow(iris)
E <- Y - X %*% B
Sigma <- crossprod(E)/nrow(iris)
Sigma

# Correlation matrix
cov2cor(Sigma)
```

```{r}
# Change the 3rd color of the palette used for the ellipses.
oldpal <- palette()
palette(c("black", "blue", "red"))
scatterplotMatrix(E,
                  gap = 0,
                  smooth = FALSE,
                  reg.line = FALSE,
                  ellipse = TRUE,
                  diagonal = "qqplot")
palette(oldpal)
```

## Model-based approach

  * Let
    $\mathcal{Y} = (Y_1^{'}, \ldots, Y_R^{'})^{'}$
    be the stacked vector $(NR \times 1)$ of the outcome matrix
    $\mathbf{Y}_{N \times R}$ by columns.
  * Let
    $\mathbf{X} = \mathrm{Bdiag}(\mathrm{X}_1, \ldots, \mathrm{X}_R)$
    be an $(NR \times k)$ design matrix, where the operator
    $\mathrm{Bdiag}$ denotes a block diagonal matrix whose entries are
    given by $\mathrm{X}$.
  * We can easily extend to the case where $\mathrm{X}$ is different for each
    response variable.
  * Let $\boldsymbol{\beta} = (\beta_1^{'}, \ldots,
    \beta_R^{'})^{'}$ be the stacked vector $(K \times
    1)$ of the regression parameters matrix $\mathbf{B}$ by columns.
  * General linear model
    $$
      \mathcal{Y} \sim \mathrm{N}_{NR}(\mathbf{X}\boldsymbol{\beta},
      \Sigma \otimes \mathrm{I}_N)
    $$
    where $\mathrm{I}_N$ denotes an $(N \times N)$ identity matrix and
    $\otimes$ denotes the Kronecker product.
  * Equivalently, we have
    $$
      \mathcal{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}
    $$
    where $\boldsymbol{\epsilon} \sim
    \mathrm{N}_{NR}(\boldsymbol{0}, \Sigma \otimes \mathrm{I})$.
  * It can be shown that
    $$
    \begin{aligned}
      \mathrm{E}(\mathcal{Y}) &= \mathbf{X}\boldsymbol{\beta},\\
      \mathrm{var}(\mathcal{Y}) &= \Sigma \otimes \mathrm{I}.
    \end{aligned}
    $$
  * For simplicity denote $\Omega = \Sigma \otimes \mathrm{I}.$

## Maximum likelihood estimation

  * Likelihood function
    $$
      L(\boldsymbol{\beta}, \Omega) =
        (2\pi)^{ -\frac{NR}{2} } |\Omega|^{-\frac{1}{2}}
        \exp \left\{
          -\frac{1}{2}
          (\mathcal{Y} - \mathbf{X}\boldsymbol{\beta})^{'}
          \Omega^{-1}
          (\mathcal{Y} - \mathbf{X}\boldsymbol{\beta})
          \right\}.
    $$
  * Log-likelihood function
    $$
      l(\boldsymbol{\beta}, \Omega) =
        -\frac{NR}{2}\log(2\pi)
        -\frac{1}{2}\log |\Omega|
        -\frac{1}{2}(
          \mathcal{Y} - \mathbf{X}\boldsymbol{\beta})^{'}
          \Omega^{-1}
          (\mathcal{Y} - \mathbf{X}\boldsymbol{\beta}).
    $$
  * Score function for $\boldsymbol{\beta}$ is given by
    $$
      U_{\boldsymbol{\beta}}(\boldsymbol{\beta}, \Omega) =
        \frac{\partial l(\boldsymbol{\beta}, \Omega)}{
          \partial \boldsymbol{\beta}} =
        \mathbf{X}^{'} \Omega^{-1}
        (\mathcal{Y} - \mathbf{X}\boldsymbol{\beta}).
    $$
  * Maximum likelihood estimator (MLE) for $\boldsymbol{\beta}$
    $$
      \hat{\boldsymbol{\beta}} =
        (\mathbf{X}^{'}\mathbf{X})^{-1}\mathbf{X}^{'}\mathcal{Y}.
    $$
  * Fisher Information Matrix for $\boldsymbol{\beta}$
    $$
      \mathcal{F}_{\boldsymbol{\beta}} =
        \mathbf{X}^{'}\Omega^{-1}\mathbf{X}.
    $$

---

  - Let $\boldsymbol{\sigma}$ be the stacked vector $(Q \times 1)$ of
    the covariance parameters matrix $\Omega$ by columns.
  - We introduce the notation $\Omega(\boldsymbol{\sigma})$ to emphasize
    that $\Omega$ is composed by the components of
    $\boldsymbol{\sigma}$.
  - Score function for the component $\sigma_{i}$ is given by
    $$
    \begin{aligned}
      U_{\sigma_{i}}(\boldsymbol{\beta}, \Omega(\boldsymbol{\sigma})) &=
      \frac{
        \partial l(\boldsymbol{\beta}, \Omega(\boldsymbol{\sigma}))}{
        \partial \sigma_{i}}\\
        &= -\frac{1}{2} \left\{
         \Omega(\boldsymbol{\sigma})^{-1}
         - \Omega(\boldsymbol{\sigma})^{-1}
         (\mathcal{Y} - \mathbf{X}\boldsymbol{\beta})
         (\mathcal{Y} - \mathbf{X}\boldsymbol{\beta})^{'}
         \Omega(\boldsymbol{\sigma})^{-1}
         \right\}
         \frac{
           \partial \Omega(\boldsymbol{\sigma})}{
           \partial \sigma_{i}}.
    \end{aligned}
    $$
  - After some calculation, we can show that the MLE for $\Omega$ is
    given by
    $$
      \hat{\Omega} = \hat{\Sigma} \otimes \mathrm{I},
    $$
    where $\hat{\Sigma}$ is the empirical covariance matrix.
  - The $(i, j)$ entry of the Fisher Information matrix for the
    components $\sigma_i$ and $\sigma_j$ is given by
    $$
      \mathcal{F}_{\sigma_i \sigma_j} =
        \frac{1}{2}\mathrm{tr}(\mathrm{W}_{\sigma_i}
        \Omega (\boldsymbol{\sigma}) \mathrm{W}_{\sigma_j}
        \Omega (\boldsymbol{\sigma})),
    $$
    where $\mathrm{W}_{\sigma_i} = -\frac{
      \partial \Omega(\boldsymbol{\sigma})^{-1}}{\partial \sigma_i}$.
  - Let $\mathcal{F}_{\boldsymbol{\sigma}}$ be the $Q \times Q$ Fisher
    information matrix for the vector $\boldsymbol{\sigma}$.
  - It can be shown that $\boldsymbol{\beta}$ and
    $\boldsymbol{\sigma}$ are orthogonal.
  - Fisher information matrix for
    $\boldsymbol{\theta} =
      (\boldsymbol{\beta}^{'}, \boldsymbol{\sigma}^{'})^{'}$
    is given by
    $$
      \mathcal{F}_{\boldsymbol{\theta}} =
      \begin{pmatrix}
        \mathcal{F}_{\boldsymbol{\beta}} & \boldsymbol{0} \\
        \boldsymbol{0} & \mathcal{F}_{\boldsymbol{\sigma}}
      \end{pmatrix}.
    $$
  - Asymptotic distribution of the MLE
    $$
      \hat{\boldsymbol{\theta}}
        \sim N_{P+Q}(\boldsymbol{\theta},
          \mathcal{F}_{\boldsymbol{\theta}}^{-1}).
    $$

##  R code

```{r, include = TRUE, message = FALSE}
# Loading data set.
data(iris)

# Fitting the model (R = 4 response variables).
fit1 <- lm(cbind(Sepal.Length,
                 Sepal.Width,
                 Petal.Length,
                 Petal.Width) ~ Species,
           data = iris)

# Estimated regression coefficients (\beta).
coef(fit1)

# Estimated covariance matrix (\Sigma).
cov(residuals(fit1))
```

  * It provides exactly the same estimates that separate calls to the
    `lm()` function.
  * Standard errors for regression coefficients are available through
    the function `vcov()`.
  * Standard errors for covariance coefficients are not available.

## Next topics

  * Reapeted measures design.
  * Profile analysis.
  * Analysis for the canonical variates.
  * Multiple comparisons procedures.
  * Checking model assumptions.
  * Multiple design linear regression models.
